{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "from scipy.sparse.linalg import eigsh\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from Baseline/cisternrois folder\n",
    "\n",
    "all_baseline_files = glob.glob(os.path.join('Baseline/secondcistern', \"*.gml\"))\n",
    "label_baseline = len(all_baseline_files) * [0]\n",
    "\n",
    "all_followup_files = glob.glob(os.path.join('Controls/secondcistern', \"*.gml\"))\n",
    "label_followup = len(all_followup_files) * [1]\n",
    "\n",
    "all_files = all_baseline_files + all_followup_files\n",
    "all_labels = label_baseline + label_followup\n",
    "\n",
    "# load graphs\n",
    "\n",
    "all_graphs = [nx.read_gml(graph) for graph in all_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_adj(adj):\n",
    "#     \"\"\" Symmetrically normalize adjacency matrix.\"\"\"\n",
    "#     \"\"\" Copy from https://github.com/tkipf/gcn \"\"\"\n",
    "#     adj = sp.coo_matrix(adj)\n",
    "#     rowsum = np.array(adj.sum(1))\n",
    "#     d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "#     d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "#     d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "#     return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(graph, max_size):\n",
    "    \"\"\"\n",
    "    Extracts eigen vector and eigen values from a graph's adjacency matrix\n",
    "    \"\"\"\n",
    "\n",
    "    adj_matrix = nx.to_numpy_array(graph)\n",
    "    # adj_matrix = normalize_adj(adj_matrix)\n",
    "    # eigen values and eigen vectors\n",
    "    eigen_values, eigen_vectors = eigh(adj_matrix)\n",
    "    print(eigen_values.shape, eigen_vectors.shape)\n",
    "\n",
    "    eigen_values, eigen_vectors = eigen_values.real, eigen_vectors.real\n",
    "\n",
    "    # eigen values to absolute values\n",
    "    eigen_values = np.abs(eigen_values)\n",
    "    print(eigen_values)\n",
    "\n",
    "    # sort eigen values and eigen vectors\n",
    "    idx = eigen_values.argsort()[::-1]\n",
    "    print(idx)\n",
    "    eigen_values = eigen_values[idx]\n",
    "    eigen_vectors = eigen_vectors[:, idx]\n",
    "\n",
    "    eigen_vectors = np.abs(eigen_vectors)\n",
    "\n",
    "    # subset top 8 eigen values and eigen vectors\n",
    "    eigen_values = eigen_values[:max_size]\n",
    "    eigen_vectors = eigen_vectors[:max_size, :max_size]\n",
    "\n",
    "    eigenvectors_flattened = eigen_vectors.flatten()\n",
    "    #print(eigenvectors_flattened)\n",
    "\n",
    "    return eigenvectors_flattened\n",
    "\n",
    "ev = [extract_features(graph, 5) for graph in all_graphs]\n",
    "X = np.array(ev)\n",
    "y = np.array(all_labels)  # Your labels array (e.g., 0 for 'bad', 1 for 'good')\n",
    "\n",
    "# Split the data into training and testing sets along with filenames\n",
    "X_train, X_test, y_train, y_test, filenames_train, filenames_test = train_test_split(\n",
    "    X, y, all_files, test_size=0.2, random_state=12, shuffle=True, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create xgboost classifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "# Get the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Average ROC\n",
    "roc = roc_auc_score(y_pred, y_test)\n",
    "print(\"ROC: {:.2f}%\".format(roc * 100))\n",
    "\n",
    "# Pair filenames in the test set with their predicted and actual labels\n",
    "results = pd.DataFrame({\n",
    "    'Filename': filenames_test,\n",
    "    'Actual Label': y_test,\n",
    "    'Predicted Label': y_pred\n",
    "})\n",
    "\n",
    "\n",
    "# Show files where the model predicted the wrong label\n",
    "print(results[results['Actual Label'] != results['Predicted Label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# fit model no training data\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"F1: %.2f%%\" % (f1 * 100.0))\n",
    "\n",
    "# Pair filenames in the test set with their predicted and actual labels\n",
    "results = pd.DataFrame({\n",
    "    'Filename': filenames_test,\n",
    "    'Actual Label': y_test,\n",
    "    'Predicted Label': y_pred\n",
    "})\n",
    "\n",
    "'''print(results)'''\n",
    "\n",
    "# Show files where the model predicted the wrong label\n",
    "print(results[results['Actual Label'] != results['Predicted Label']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(ev)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    #model = XGBClassifier()\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "\n",
    "# Output average results\n",
    "print(\"Average Accuracy: {:.2f}%\".format(avg_accuracy * 100))\n",
    "print(\"Average F1 Score: {:.2f}%\".format(avg_f1_score * 100))\n",
    "\n",
    "# Aggregate confusion matrices\n",
    "total_conf_matrix = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "# Output the aggregated confusion matrix\n",
    "print(\"Aggregated Confusion Matrix:\")\n",
    "print(total_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM Model\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'gamma': ['scale', 'auto'],  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
    "    'kernel': ['linear', 'rbf', 'poly']  # Type of the kernel\n",
    "}\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "svm_model = SVC()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(svm_model, param_grid, cv=5, scoring='f1')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Use best params to train the model\n",
    "best_svm_model = SVC(C=grid_search.best_params_['C'], gamma=grid_search.best_params_['gamma'],\n",
    "                     kernel=grid_search.best_params_['kernel'])\n",
    "best_svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X = np.array(ev)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "roc = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    model = SVC(C=1, gamma='scale', kernel='poly')\n",
    "                \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "    roc.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_roc = np.mean(roc)\n",
    "\n",
    "# Output average results\n",
    "print(\"Average Accuracy: {:.2f}%\".format(avg_accuracy * 100))\n",
    "print(\"Average F1 Score: {:.2f}%\".format(avg_f1_score * 100))\n",
    "print(\"Average ROC: {:.2f}%\".format(avg_roc * 100))\n",
    "\n",
    "# Aggregate confusion matrices\n",
    "total_conf_matrix = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "# Output the aggregated confusion matrix\n",
    "print(\"Aggregated Confusion Matrix:\")\n",
    "print(total_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_folds = 5\n",
    "\n",
    "# Set up K-Fold cross-validation\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "X = np.array(ev)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "# Initialize lists to store results\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "conf_matrices = []\n",
    "roc = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split data\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # Create and fit the model\n",
    "    model = lgb.LGBMClassifier(learning_rate=0.01, n_estimators=1000, num_leaves=31, objective='binary', metric='accuracy ')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and store metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    conf_matrices.append(confusion_matrix(y_test, y_pred))\n",
    "    roc.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_roc = np.mean(roc)\n",
    "\n",
    "# Output average results\n",
    "print(\"Average Accuracy: {:.2f}%\".format(avg_accuracy * 100))\n",
    "print(\"Average F1 Score: {:.2f}%\".format(avg_f1_score * 100))\n",
    "print(\"Average ROC: {:.2f}%\".format(avg_roc * 100))\n",
    "\n",
    "# Aggregate confusion matrices\n",
    "total_conf_matrix = np.sum(conf_matrices, axis=0)\n",
    "\n",
    "# Output the aggregated confusion matrix\n",
    "print(\"Aggregated Confusion Matrix:\")\n",
    "print(total_conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "X = np.array(ev)\n",
    "y = np.array(all_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# define objective function\n",
    "\n",
    "def objective(trial, data=X_train, target=np.ravel(y_train)):\n",
    "    train_X, test_X, train_y, test_y = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "    hyperparams = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'n_estimators': 500,\n",
    "        'seed': 2023,\n",
    "        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 0.2, 1.0),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 14),\n",
    "\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**hyperparams)\n",
    "    model.fit(train_X, train_y, eval_set=[(test_X, test_y)], callbacks=[lgb.early_stopping(10,0,0.005)])\n",
    "\n",
    "    preds = model.predict(test_X) \n",
    "\n",
    "    accuracy = roc_auc_score(test_y, preds)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "sample = TPESampler(seed=2023)\n",
    "study = optuna.create_study(direction='maximize', sampler=sample)\n",
    "study.optimize(objective, n_trials=500)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "trial = study.best_trial\n",
    "print('Accuracy: {}'.format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model with best hyperparameters\n",
    "\n",
    "lgb_params = trial.params\n",
    "lgb_params['metric'] = 'auc'\n",
    "lgb_params['random_state'] = 2023\n",
    "lgb_params['n_estimators'] = 1000\n",
    "\n",
    "m = lgb.LGBMClassifier(**lgb_params)\n",
    "m.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "preds = m.predict(X_test)\n",
    "\n",
    "print('ROC AUC score: {}'.format(roc_auc_score(y_test, preds)))\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC AUC score: {}'.format(roc_auc_score(y_test, preds)))\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print(classification_report(y_test, preds))\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n",
    "trial = study.best_trial\n",
    "print('Accuracy: {}'.format(trial.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
